#!/usr/bin/env python3
"""
–§–∏–Ω–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è —Å–∞–π—Ç–∞ erzrf.ru - –ø–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –∫–æ–º–ø–∞–Ω–∏–π
–ò–∑–≤–ª–µ–∫–∞–µ—Ç: –Ω–∞–∑–≤–∞–Ω–∏–µ, –≥–æ—Ä–æ–¥, —Å–∞–π—Ç, —Å–æ—Ü—Å–µ—Ç–∏
"""

import requests
from requests_html import HTMLSession
from bs4 import BeautifulSoup
import pandas as pd
import time
import re
from urllib.parse import urljoin
import json
from datetime import datetime

class FinalERZRFParser:
    def __init__(self):
        # –û–±—ã—á–Ω–∞—è —Å–µ—Å—Å–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Å—ã–ª–æ–∫
        self.requests_session = requests.Session()
        self.requests_session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
        
        # HTML —Å–µ—Å—Å–∏—è –¥–ª—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ JavaScript
        self.html_session = HTMLSession()
        
        self.base_url = 'https://erzrf.ru'
        self.companies_data = []
        self.failed_urls = []
        
    def get_company_links_from_page(self, page_num):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∫–æ–º–ø–∞–Ω–∏–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Ç–æ–ø–∞"""
        url = f"{self.base_url}/top-zastroyshchikov/rf?regionKey=0&topType=0&date=250801&page={page_num}"
        print(f"–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {url}")
        
        try:
            response = self.requests_session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –∫–æ–º–ø–∞–Ω–∏–∏
            company_links = []
            links = soup.find_all('a', href=re.compile(r'/zastroyschiki/brand/'))
            
            for link in links:
                href = link.get('href')
                if href:
                    full_url = urljoin(self.base_url, href)
                    company_links.append(full_url)
            
            # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            company_links = list(set(company_links))
            print(f"–ù–∞–π–¥–µ–Ω–æ {len(company_links)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page_num}")
            
            return company_links
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page_num}: {e}")
            return []
    
    def extract_company_data(self, company_url, attempt=1):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ –µ—ë –∫–∞—Ä—Ç–æ—á–∫–∏"""
        print(f"–ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–º–ø–∞–Ω–∏–∏ (–ø–æ–ø—ã—Ç–∫–∞ {attempt}): {company_url}")
        
        try:
            r = self.html_session.get(company_url, timeout=30)
            
            # –†–µ–Ω–¥–µ—Ä–∏–º JavaScript —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º –≤—Ä–µ–º–µ–Ω–µ–º –æ–∂–∏–¥–∞–Ω–∏—è
            print("–†–µ–Ω–¥–µ—Ä–∏–º JavaScript –¥–ª—è –∫–æ–º–ø–∞–Ω–∏–∏...")
            r.html.render(timeout=30, wait=10, sleep=2)
            
            company_data = {
                '–Ω–∞–∑–≤–∞–Ω–∏–µ': '',
                '–≥–æ—Ä–æ–¥': '',
                '—Å–∞–π—Ç': '',
                '—Å–æ—Ü—Å–µ—Ç–∏': '',
                'url_–∫–∞—Ä—Ç–æ—á–∫–∏': company_url
            }
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–∏
            title_found = False
            
            # –°–ø–æ—Å–æ–± 1: –ò–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            try:
                title = r.html.find('title')
                if title:
                    title_text = title[0].text
                    if title_text and ' - ' in title_text:
                        name = title_text.split(' - ')[0].strip()
                        if name and len(name) < 200 and '–ï–†–ó' not in name and name != '–ü–†–û–§–ò-–ò–ù–í–ï–°–¢':
                            company_data['–Ω–∞–∑–≤–∞–Ω–∏–µ'] = name
                            title_found = True
                        elif name == '–ü–†–û–§–ò-–ò–ù–í–ï–°–¢':
                            company_data['–Ω–∞–∑–≤–∞–Ω–∏–µ'] = name
                            title_found = True
            except:
                pass
            
            # –°–ø–æ—Å–æ–± 2: –ò—â–µ–º –≤ HTML —ç–ª–µ–º–µ–Ω—Ç–∞—Ö
            if not title_found:
                title_selectors = ['h1', '.org-name', '.company-name', '.brand-name', '[class*="title"]']
                
                for selector in title_selectors:
                    try:
                        elements = r.html.find(selector)
                        if elements:
                            title_text = elements[0].text.strip()
                            if title_text and len(title_text) < 200 and '–ï–†–ó' not in title_text:
                                company_data['–Ω–∞–∑–≤–∞–Ω–∏–µ'] = title_text
                                title_found = True
                                break
                    except:
                        continue
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–∞–π—Ç–∞
            site_found = False
            
            # –°–ø–æ—Å–æ–± 1: –ò—â–µ–º –≤ HTML –∫–æ–¥–µ –ø–∞—Ç—Ç–µ—Ä–Ω —Å —Å–∞–π—Ç–æ–º
            try:
                html_content = r.html.html
                
                # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–∞–π—Ç–∞
                site_patterns = [
                    r'<div[^>]*id="org-site"[^>]*>.*?<a[^>]*href="([^"]*)"',
                    r'"–°–∞–π—Ç".*?href="([^"]*)"',
                    r'–°–∞–π—Ç.*?<a[^>]*href="([^"]*)"'
                ]
                
                for pattern in site_patterns:
                    matches = re.findall(pattern, html_content, re.DOTALL | re.IGNORECASE)
                    for match in matches:
                        if (match and 'http' in match and 'erzrf.ru' not in match and 
                            'metrika.yandex' not in match and 'uniteddevelopers' not in match):
                            company_data['—Å–∞–π—Ç'] = match
                            site_found = True
                            break
                    if site_found:
                        break
            except:
                pass
            
            # –°–ø–æ—Å–æ–± 2: –ò—â–µ–º —Å—Ä–µ–¥–∏ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫
            if not site_found:
                try:
                    all_links = r.html.find('a[href]')
                    excluded_domains = [
                        'erzrf.ru', 'vk.com', 'facebook.com', 'instagram.com', 
                        'twitter.com', 't.me', 'youtube.com', 'ok.ru',
                        'uniteddevelopers.ru', 'metrika.yandex.ru', 'google.com'
                    ]
                    
                    for link in all_links:
                        href = link.attrs.get('href', '')
                        if (href and 'http' in href and 
                            not any(domain in href.lower() for domain in excluded_domains) and
                            any(tld in href for tld in ['.ru', '.com', '.—Ä—Ñ', '.org', '.net'])):
                            company_data['—Å–∞–π—Ç'] = href
                            site_found = True
                            break
                except:
                    pass
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å–æ—Ü—Å–µ—Ç–µ–π
            social_networks = []
            
            try:
                # –ò—â–µ–º –≤ HTML –∫–æ–¥–µ –ø–∞—Ç—Ç–µ—Ä–Ω —Å —Å–æ—Ü—Å–µ—Ç—è–º–∏
                html_content = r.html.html
                
                # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ—Ü—Å–µ—Ç–µ–π
                social_patterns = [
                    r'<app-org-social[^>]*>.*?href="([^"]*vk\.com[^"]*)"',
                    r'<app-org-social[^>]*>.*?href="([^"]*facebook[^"]*)"',
                    r'<app-org-social[^>]*>.*?href="([^"]*instagram[^"]*)"',
                    r'<div[^>]*id="org-social"[^>]*>.*?href="([^"]*)"'
                ]
                
                for pattern in social_patterns:
                    matches = re.findall(pattern, html_content, re.DOTALL | re.IGNORECASE)
                    for match in matches:
                        if (match and any(social in match.lower() for social in ['vk.com', 'facebook', 'instagram']) and
                            'erzrf_ru' not in match and '+kf4d9SlCRpRiMGQy' not in match):
                            if match not in social_networks:
                                social_networks.append(match)
            except:
                pass
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –±–ª–æ–∫–∞—Ö, –∏—â–µ–º –ø–æ –≤—Å–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü–µ
            if not social_networks:
                social_domains = ['vk.com', 'facebook.com', 'instagram.com', 'twitter.com', 't.me', 'youtube.com', 'ok.ru']
                
                try:
                    all_links = r.html.find('a[href]')
                    for link in all_links:
                        href = link.attrs.get('href', '')
                        if href:
                            for domain in social_domains:
                                if (domain in href.lower() and 
                                    href not in social_networks and
                                    'erzrf_ru' not in href and
                                    '+kf4d9SlCRpRiMGQy' not in href):
                                    social_networks.append(href)
                                    break
                except:
                    pass
            
            company_data['—Å–æ—Ü—Å–µ—Ç–∏'] = '; '.join(social_networks) if social_networks else ''
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≥–æ—Ä–æ–¥–∞
            city_found = False
            page_text = r.html.text
            
            # –ò—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            city_patterns = [
                r'–≥\.\s*([–ê-–Ø–∞-—è—ë][–ê-–Ø–∞-—è—ë\s\-]{2,25})',
                r'–≥–æ—Ä–æ–¥\s+([–ê-–Ø–∞-—è—ë][–ê-–Ø–∞-—è—ë\s\-]{2,25})',
                r'([–ê-–Ø–∞-—è—ë][–ê-–Ø–∞-—è—ë\s\-]{2,25})\s*–æ–±–ª–∞—Å—Ç—å',
                r'([–ê-–Ø–∞-—è—ë][–ê-–Ø–∞-—è—ë\s\-]{2,25})\s*–∫—Ä–∞–π',
                r'([–ê-–Ø–∞-—è—ë][–ê-–Ø–∞-—è—ë\s\-]{2,25})\s*—Ä–µ—Å–ø—É–±–ª–∏–∫–∞'
            ]
            
            for pattern in city_patterns:
                matches = re.findall(pattern, page_text, re.IGNORECASE)
                for match in matches:
                    city = match.strip()
                    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–ª–æ—Ö–∏–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è
                    if (len(city) >= 3 and len(city) <= 25 and 
                        not any(word in city.lower() for word in [
                            '–∑–∞—Å—Ç—Ä–æ–π—â–∏–∫', '–∫–æ–º–ø–∞–Ω–∏—è', '–≥—Ä—É–ø–ø–∞', '—Å—Ç—Ä–æ–∏—Ç–µ–ª—å—Å—Ç–≤–æ', 
                            '–∏–Ω–≤–µ—Å—Ç', '–¥–µ–≤–µ–ª–æ–ø–º–µ–Ω—Ç', '–ø—Ä–æ–µ–∫—Ç', '—É—á—Ä–µ–¥–∏—Ç–µ–ª—å',
                            '–∞–ª—Ç–∞–π—Å–∫–∏–π', '–∞–º—É—Ä—Å–∫–∞—è', '–∞—Ä—Ö–∞–Ω–≥–µ–ª—å—Å–∫–∞—è'
                        ]) and
                        city.lower() not in ['–º–æ—Å–∫–≤–∞', '—Å–∞–Ω–∫—Ç', '–ø–µ—Ç–µ—Ä–±—É—Ä–≥', '—Ä–æ—Å—Å–∏–π—Å–∫–∞—è']):
                        company_data['–≥–æ—Ä–æ–¥'] = city
                        city_found = True
                        break
                if city_found:
                    break
            
            return company_data
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ –∫–æ–º–ø–∞–Ω–∏–∏ {company_url}: {e}")
            # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø–æ–ø—ã—Ç–∫–∞
            if attempt < 2:
                time.sleep(5)
                return self.extract_company_data(company_url, attempt + 1)
            else:
                self.failed_urls.append(company_url)
                return None
    
    def parse_all_pages(self, max_pages=15):
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Ç–æ–ø–∞"""
        print("–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü...")
        print(f"–í—Ä–µ–º—è –Ω–∞—á–∞–ª–∞: {datetime.now()}")
        
        all_company_links = []
        
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å–æ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        for page_num in range(1, max_pages + 1):
            links = self.get_company_links_from_page(page_num)
            if not links:
                print(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_num} –ø—É—Å—Ç–∞, –∑–∞–≤–µ—Ä—à–∞–µ–º —Å–±–æ—Ä —Å—Å—ã–ª–æ–∫")
                break
            all_company_links.extend(links)
            time.sleep(1)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ç—Ä–∞–Ω–∏—Ü–∞–º–∏
        
        print(f"–í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ {len(all_company_links)} –∫–æ–º–ø–∞–Ω–∏–π")
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        all_company_links = list(set(all_company_links))
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π: {len(all_company_links)}")
        
        # –ü–∞—Ä—Å–∏–º –¥–∞–Ω–Ω—ã–µ –∫–∞–∂–¥–æ–π –∫–æ–º–ø–∞–Ω–∏–∏
        for i, company_url in enumerate(all_company_links, 1):
            print(f"\n{'='*60}")
            print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–æ–º–ø–∞–Ω–∏—é {i}/{len(all_company_links)}")
            print(f"–í—Ä–µ–º—è: {datetime.now()}")
            
            company_data = self.extract_company_data(company_url)
            if company_data:
                self.companies_data.append(company_data)
                print(f"‚úì –î–æ–±–∞–≤–ª–µ–Ω–∞: {company_data['–Ω–∞–∑–≤–∞–Ω–∏–µ']}")
                if company_data['—Å–∞–π—Ç']:
                    print(f"  –°–∞–π—Ç: {company_data['—Å–∞–π—Ç']}")
                if company_data['–≥–æ—Ä–æ–¥']:
                    print(f"  –ì–æ—Ä–æ–¥: {company_data['–≥–æ—Ä–æ–¥']}")
                if company_data['—Å–æ—Ü—Å–µ—Ç–∏']:
                    print(f"  –°–æ—Ü—Å–µ—Ç–∏: {company_data['—Å–æ—Ü—Å–µ—Ç–∏'][:100]}...")
            else:
                print(f"‚úó –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ")
            
            # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(3)
            
            # –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 20 –∫–æ–º–ø–∞–Ω–∏–π
            if i % 20 == 0:
                self.save_to_excel(f'companies_backup_{i}.xlsx')
                print(f"–ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ: {len(self.companies_data)} –∫–æ–º–ø–∞–Ω–∏–π")
        
        return self.companies_data
    
    def save_to_excel(self, filename='erzrf_companies_final.xlsx'):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ Excel"""
        if not self.companies_data:
            print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return
        
        df = pd.DataFrame(self.companies_data)
        df.to_excel(filename, index=False, engine='openpyxl')
        print(f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
        print(f"–í—Å–µ–≥–æ –∫–æ–º–ø–∞–Ω–∏–π: {len(df)}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print(f"–ö–æ–º–ø–∞–Ω–∏–π —Å —Å–∞–π—Ç–∞–º–∏: {df[df['—Å–∞–π—Ç'] != ''].shape[0]}")
        print(f"–ö–æ–º–ø–∞–Ω–∏–π —Å —Å–æ—Ü—Å–µ—Ç—è–º–∏: {df[df['—Å–æ—Ü—Å–µ—Ç–∏'] != ''].shape[0]}")
        print(f"–ö–æ–º–ø–∞–Ω–∏–π —Å –≥–æ—Ä–æ–¥–∞–º–∏: {df[df['–≥–æ—Ä–æ–¥'] != ''].shape[0]}")
        
        if self.failed_urls:
            print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: {len(self.failed_urls)} –∫–æ–º–ø–∞–Ω–∏–π")
    
    def save_to_csv(self, filename='erzrf_companies_final.csv'):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ CSV"""
        if not self.companies_data:
            print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return
        
        df = pd.DataFrame(self.companies_data)
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
    
    def save_failed_urls(self, filename='failed_urls.txt'):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –Ω–µ—É–¥–∞—á–Ω—ã—Ö URL"""
        if self.failed_urls:
            with open(filename, 'w', encoding='utf-8') as f:
                for url in self.failed_urls:
                    f.write(url + '\n')
            print(f"–°–ø–∏—Å–æ–∫ –Ω–µ—É–¥–∞—á–Ω—ã—Ö URL —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {filename}")
    
    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–µ—Å—Å–∏–π"""
        if hasattr(self, 'html_session'):
            self.html_session.close()

def main():
    parser = FinalERZRFParser()
    
    print("üöÄ –ó–ê–ü–£–°–ö –§–ò–ù–ê–õ–¨–ù–û–ì–û –ü–ê–†–°–ï–†–ê ERZRF.RU")
    print("–ü–∞—Ä—Å–∏–º –¢–û–ü-250 –∑–∞—Å—Ç—Ä–æ–π—â–∏–∫–æ–≤ –†–æ—Å—Å–∏–∏")
    print("="*60)
    
    try:
        # –ü–∞—Ä—Å–∏–º –≤—Å–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        companies = parser.parse_all_pages(max_pages=15)  # 250 –∫–æ–º–ø–∞–Ω–∏–π ‚âà 13 —Å—Ç—Ä–∞–Ω–∏—Ü
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        parser.save_to_excel('erzrf_companies_final.xlsx')
        parser.save_to_csv('erzrf_companies_final.csv')
        parser.save_failed_urls('failed_urls.txt')
        
        print(f"\nüéâ –ü–ê–†–°–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù!")
        print(f"–í—Ä–µ–º—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è: {datetime.now()}")
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∫–æ–º–ø–∞–Ω–∏–π: {len(companies)}")
        
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        if parser.companies_data:
            parser.save_to_excel('erzrf_companies_partial_final.xlsx')
            parser.save_to_csv('erzrf_companies_partial_final.csv')
            parser.save_failed_urls('failed_urls_partial.txt')
            print("–ß–∞—Å—Ç–∏—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã")
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        if parser.companies_data:
            parser.save_to_excel('erzrf_companies_error_final.xlsx')
            parser.save_to_csv('erzrf_companies_error_final.csv')
            parser.save_failed_urls('failed_urls_error.txt')
            print("–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –¥–æ –æ—à–∏–±–∫–∏")
    finally:
        parser.close()

if __name__ == "__main__":
    main()